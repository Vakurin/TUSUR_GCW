Министерство науки и высшего образования российской федерации    

Федеральное государственное бюджетное образовательное 
учреждение высшего образования

ТОМСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ СИСТЕМ УПРАВЛЕНИЯ И РАДИОЭЛЕКТРОНИКИ (ТУСУР)

Кафедра автоматизированных систем управления (АСУ)

К ЗАЩИТЕ ДОПУСТИТЬ
    И.о. зав. кафедрой АСУ
канд. техн. наук, доцент  
___________ В.В. Романенко
     «___» __________20      г.

Разработка синтетических данных с помощью генеративных-состязательных сетей
Бакалаврская работа
 по направлению 09.03.01 «Информатика и вычислительная техника»
 

Консультант 
рук. отд. разработки маш. обучения, ООО «Викью Софт», к.т.н., 
_______________ И.К. Идрисов
«___» _________ 2020 г.








Студент гр. 436-1
                           М.А. Вакурин

«___» _________ 2020 г.

Руководитель
Проф. каф. АСУ, к.т.н,
                           В.Д. Сибилев
«___» _________ 2020 г.


 
Томск 2020           
Министерство науки и высшего образования российской федерации
Федеральное государственное бюджетное образовательное 
учреждение высшего образования
ТОМСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ СИСТЕМ
УПРАВЛЕНИЯ И РАДИОЭЛЕКТРОНИКИ (ТУСУР)
Кафедра автоматизированных систем управления (АСУ)

УТВЕРЖДАЮ
   И.о. зав. кафедрой АСУ
канд. техн. наук, доцент  
__________ В.В. Романенко
«___» ____________2020 г.


ЗАДАНИЕ

к бакалаврской работе студенту Вакурину Максиму Алексеевичу, группа 
436 - 1, факультет систем управления
1 Тема бакалаврской работы (БР): «Разработка синтетических данных с помо-щью генеративных-состязательных сетей» 
(утверждена приказом по ВУЗу от «___» ____________2020 г. № _______)
2 Срок сдачи студентом законченной работы « 3 »    июля       2020 г.
3 Исходные данные к работе:
3.1 Техническое задание.
3.2 Список литературных источников по теме ВКР.
3.3 ОС ТУСУР 01–2013. Работы студенческие по направлениям подготов-ки и специальностям технического профиля. Общие требования и пра¬вила оформления.
 
4 Содержание расчетно-пояснительной записки / перечень подлежащих раз-работке вопросов:
4.1 Обзор методов генерации
4.2 Разработка программной системы

5 Перечень графического материала:
-	актуальность;
-	цель и задачи;
-	особенности глубокого обучения при обработке изображения;
-	применение GAN-сетей для генерации синтетических образцов;
-	используемые программные системы;
-	способ получения новых данных;
-	введение в генеративные сети;
-	результаты генерации;
-	заключение.
6. Дата выдачи задания «15»        июня      2020 г.
Руководитель выпускной квалификационной работы

Доцент кафедры АСУ ТУСУР,  к.т.н.,          
«         »                         2020 г.                           _____________ В.Д. Сибилев

Консультант 
рук. отд. разработки маш. обучения, 
ООО «Викью Софт», к.т.н.,         
«         »                         2020 г.                           _____________ И.К. Идрисов

Задание принято к исполнению 
«15»           июня          2020 г. 		      ______________М.А. Вакурин 
Реферат

	Пояснительная записка к бакалаврской работе содержит 39 страниц, 26 рисунков, 19 источников.
	ГЛУБОКОЕ ОБУЧЕНИЕ, СИНТЕТИЧЕСКИЕ ДАННЫЕ, GAN-СЕТИ, ГЕНЕРАТИВНО-СОСТЯЗАТЕЛЬНЫЕ СЕТИ.
Объектом данной работы является разработка системы для генерации син-тетических данных с помощью генеративных-состязательных сетей.
	Целью работы – получение системы, которая по одному входному изоб-ражению может создавать на выходе изображения разных размеров.
	Результатом разработки является сеть глубокого обучения, позволяющая создавать новые изображения, где на вход подаем одно изображение, а на выхо-де получаем сгенерированные изображения с разным соотношением сторон. 
	Программная система реализована на языке программирования Python 3 с использованием библиотеки глубокого обучение PyTorch и с помощью сервиса для тренировки моделей Google Colab. Среда разработки – Jupyter Notebook, операционная система – Mac OS 10.15.
	Область применения – компании, заинтересованные в показе контента на разные устройства. 
	Пояснительная записка к ВКР выполнена в текстовом редакторе MS Word 2016.


 
Abstract
	The explanatory note to the final qualifying work contains 39 pages, 26 figures, 19 sources.
	DEEP LEARNING, SYNTHETIC DATA, GAN-NETWORKS, GENERA-TIVE ADVERSARIAL NETWORK.
	The object of this work is to develop a system for generated synthetic data with generative adversarial network.
	The aim of the work is to develop a system for generation synthetic data with GAN-Network.	
The result of development is a deep learning network that allows you to gener-ate new image by source image and generating new samples of arbitrary size and as-pect ratio.
	The software system is implemented in the Python 3 is a programming lan-guage which using additional framework PyTorch is a deep learning library and for training models Google Collaboratory. The development environment is Jupyter Notebook, the operating system is Mac OS 10.15.
	Explanatory note to the WRC is made in the text editor MS Word 2016.


 
Содержание
Введение	4
1   Обзор предметной области	5
1.1   Генеративное моделирование	5
1.2   Глубокое обучение	6
1.3   Генеративно-состязательные сети	7
1.4   Синтетические данные	9
1.5   Применение GAN и синтетических данных	10
1.6   Среда разработки	12
1.7   Язык разработки	13
2   Основные компоненты глубокого обучения	15
2.1   SinGAN	15
2.2   Архитектура сети	15
2.3   Архитектура Генератора	17
2.4   Архитектура Дискриминатора	18
2.5   Функция потерь	19
3   Разработка	20
3.1   Постановка задачи	20
3.2   Реализация архитектуры	20
3.3   Реализация штрафов градиента	21
3.4   Реализация одного шага обучения	23
3.5   Реализация тренировки	26
3.6   Реализация анимации	27
3.7   Реализация генерации изображений	28
3.8   Реализация картинок с разным соотношением сторон	29
3.9   Результаты	30
Заключение	34
Список использованных источников	35

Введение
        С каждым годом у нас становится все больше устройств и у каждого такого устройства свое соотношение сторон, из-за этого все сложнее делать контент, так как разработчикам и создателям контента приходится думать о десятках разных размеров.
Целью бакалаврской работы выбрана разработка сети глубокого обуче-ния, которая смогла бы получать на входе изображение одного размера и гене-рировать изображения с разными размерами, не теряя при этом необходимую информацию.
Следуя поставленной цели, в качестве основы, в данной работе выбрана технология Глубокого Обучения (Deep Learning), в ходе реализации которой планируется разработать инновационный принцип программной обработки по-ступающих на вход изображений одного размера и последующей генерации изображений других размеров на выходе.
Целью выпускной квалификационной работы является разработка про-граммной системы генерации изображений с разным соотношением сторон по одному входному изображению, но других размеров. Для решения поставлен-ной цели были выполнены следующие задачи:
1)	изучены особенности глубоких и генеративных сетей;
2)	выполнен обзор по методам применения генеративных сетей;
3)	разработана архитектура сети, применимой для сетей глубокого обучения, которая принимает на вход одно изображение, а на выходе генерирует несколь-ко изображений разных размеров по сравнению с размерами оригинала;
4)	проведен полный разбор всех слоев, которые нужны для реализации сети глубокого обучения;
5)	проведено тестирование по разработанной модели глубокого обучения.

1   Обзор предметной области

1.1   Генеративное моделирование
Один из ярких примеров генеративного моделирования [1] — это челове-ческое воображение. Человек может закрыть глаза и представить все, что ему захочется. Нейробиологи предполагают, что наше восприятие реальности осно-вывается на генеративном моделировании – с рождения мы можем моделиро-вать наше окружение, которое соответствует настоящему.
Генеративное моделирование можно описать так: у нас есть набор дан-ных, которые генерируются с точки зрения вероятностной модели. Используя эту модель, можно получить новые данные. Если хотим создать новое изобра-жение, то нам нужен будет обучающий набор с похожим классом изображений. Целью является создание модели, способной генерировать признаки, выглядя-щие настоящими. В задачах генерации роль признаков обычно являются пиксе-ли картинки.
Основные математические обозначения в генеративном моделировании описывают через p(x) - вероятность получения наблюдения Х. Если набор дан-ных содержит метки, то можно построить генеративную модель, оценивающую распределение p(x|y). В итоге модель не учитывает метки наблюдения и оцени-вает вероятность того, что сгенерированный образец похож на остальные наблюдения.
Базовые принцип генеративного моделирования:
-	Имеется набор изображений X
-	Предполагается, что изображения сгенерированы с некоторым неиз-вестным распределением p_data.
-	Генеративная модель p_model пытается имитировать p_data. С её по-мощью можем генерировать наблюдения, которые выглядят похожи-ми на p_data.
-	p_model работает правильно, если: 
1.	Генерирует образцы, которые выглядят аналогично полученным из p_data
2.	Если сгенерированные образцы, отличаются от изображений X, то есть она не должна их копировать.
Этим основным принципам и следует генеративное моделирование.
1.2   Глубокое обучение
Методы глубокого обучение [2] возникли еще в 1980-ом году, но с увели-чением объема данных и ростом вычислительных мощностей, эта сфера начала набирать популярность и появились первые результаты. Многие алгоритмы стали осуществимы и начали превосходить то, что было до их реализации.
Модели глубокого обучения способны самостоятельно выявлять высоко-уровневые информативные признаки из неструктурированных данных (аудио, видео, фотографий). Когда речь заходит о генерации данных, то сфера глубоко-го обучения показывает всю силу, поэтому основные принципы Deep Learning используются в генеративном моделировании.
Большинство систем глубокого обучения представляют собой нейронные сети с несколькими скрытыми слоями, наложенными друг на друга. В общих чертах, каждый слой содержит нейроны, связанные с узлами в соседнем слое посредством набора весов (весовых коэффициентов). Чаще всего используются "полносвязные" слои, которые соединяются друг с другом. Глубокие сети могут иметь любое количество скрытых слоев. Сила глубоких сетей заключается в их способности находить наборы весов для каждого слоя, дающего наиболее точ-ные прогнозы. Процесс поиска весов называется обучением.
Основным "топливом" для глубокого обучения становятся данные: это могут быть аудио, видео материалы, фотографии, рисунки и т. п. Благодаря данным, сеть способна находить повторяющиеся элементы и извлекать из них признаки, которые помогут в работе сети.
С каждым годом данных становится все больше, однако при этом полу-чить их все труднее. К примеру, недавний европейский закон о защите персо-нальных данных (GDPR) [3], в котором обработка данных стала юридической процедурой и теперь просто так не собрать данные о пользователе, нужен акт о согласии пользователя на обработку его данных. Кроме того, данные должны быть зашифрованными, а передача третьим лицам – запрещена. Компании обя-заны иметь сотрудника, ответственного за безопасность этих данных (data protection officer). Если личные данные пользователей будут утеряны или раз-глашены, то это грозит компаниям, допустившим подобное, многомиллионны-ми штрафными санкциями. Развитие глубокого обучения может замедлиться из-за бюрократических обязательств, и все меньше компаний будут заинтересо-ваны в разработке проектов, основанных на нейронных сетях.
1.3   Генеративно-состязательные сети
Одна из самых развивающихся областей глубокого обучения называется GAN (Generative Adversarial Networks) или генеративные состязательные сети – это такие сети, которые способны генерировать или восстанавливать любой вид медиа файла, начиная с окраски черно-белого изображения в цветное, заканчи-вая созданием приятной человеческому восприятию музыки. Каждый месяц вы-ходят десятки статей о GAN-сетях, лучшие умы человечества работают в этой области, многие издания считают, что это самая прорывная и перспективная сфера за последние 10 лет.
В 2016 году, на конференции Neural Informatiom Processing Systems (NIPS) в Барселоне, Ян Гудфеллоу [4], руководитель отдела глубокого обучения из Google Brains, представил доклад под названием "Generative Adversatial Network" [5]. Его идеи, освещенные в той работе, в настоящее время рассматри-ваются как один из поворотных моментов в развитии генеративного моделиро-вания, и с тех пор появилось множество вариаций решений на тему генератив-ных сетей.
Генеративно-состязательные сети — это борьба между генератором и дис-криминатором. Генератор пытается преобразовать случайный шум в изображе-ние или любой медиа файл, который должен выглядит так, будто выбран из ре-альных образцов данных. Дискриминатор пытается понять, являются ли предо-ставленные ему данные настоящими или фейковым (фальшивыми).
В начале процесса генератор выводит искаженные изображения, а дис-криминатор оценивает их случайным образом. Основная особенность GAN - попеременное обучение двух подсетей: генератор постепенно совершенствуется в обмане дискриминатора, а дискриминатор, адаптируясь, учится правильно определять фальшивые наблюдения, что заставляет генератор искать новые спо-собы обмана дискриминатора.
 
Рисунок 1.1 - Архитектура GAN [6]
1.4   Синтетические данные
Глубокое обучение набирает все больше и больше популярности в наше время, одним из важнейших факторов такой популярности являются данные. В качестве примеров источников возникновения данных приводятся непрерывно поступающие данные с измерительных устройств, потоки сообщений из социальных сетей, метеорологические данные, потоки данных о местонахож-дении абонентов сетей сотовой связи, устройств аудио- и видео-регистрации. Эти данные становятся “Мотором” для глубокого обучения, позволяя обучать модель, предсказывать результаты по наборам входных данных.
С непрерывно растущим объемом данных возникает проблема их обра-ботки. Для многих задач нужна разметка или маска в данных, а чтобы разметить данные требуется много часов работы человека, ведь как правило зачастую это делается вручную. Из-за этого разметка данных становится достаточно затрат-ным процессом. Как пример невозможности получения исходных для обработ-ки данных – это необходимость дополнительного использования специальных устройств (датчиков, сенсоров, приборов ночного видения и т. п.), без которых, например, получить реальную глубину изображения практически невозможно. Поэтому используются синтетические данные, когда достаточно создать 3D мо-дель и её окружение и поместить это в среду разработки, в результате чего эму-лируется искомая разметка контролируемым программным процессом.
На практике возможно создавать 3D-модели интересующих нас объектов (вручную) и помещать их в необходимое виртуальное окружение, а затем воз-можно генерировать картинки, как рендеры 3D-сцен с различных точек зрения. Таким образом на выходе могут быть сгенерированы изображения, как кадры из мультфильма, и при этом вполне возможно «запечатлеть» необходимые объек-ты со всех сторон, в любых комбинациях и ракурсах. Важно, что разметка мето-дами синтетической генерации получается автоматически и абсолютно идеаль-но. Самостоятельно создав 3D-сцену и разместив камеру, мы в состоянии без-ошибочно понимать, к какому объекту относится буквально каждый пиксель полученного рендера.
 
Рисунок 1.2 - Пример синтетических данных
Синтетические данные особенно хорошо работают в задачах компьютер-ного «зрения» [7]. То есть в задачах, где искусственному интеллекту нужно рас-познавать объекты на фотографиях или в окружающем его мире в режиме ре-ального времени. Впервые синтетические данные начали применять в беспи-лотных автомобилях, создавая программные симуляции вождения задолго до тестов на реальных дорогах и с реальными людьми.
1.5   Применение GAN и синтетических данных
Многие крупные IT-компании активно исследуют GAN-сети, существует множество примеров удачных решений. Одним из таких примером, стала сеть от компании Nvidia, которая называется StyleGAN2 [8] – эта сеть создает лица людей, которых не существует в реальности. Они даже сделали отдельных сайт This Person Does't Exist [9], где можно посмотреть на сгенерированные лица с помощью нейронной сети. Это впечатляющие результаты, чтобы понять, что эти сгенерированные картинки отличить от настоящих практически невозмож-но.
 
Рисунок 1.3 - Все лица сгенерированы с помощью StyleGAN2 и их не существу-ет
Компания OpenAI [10], занимающаяся разработкой искусственного ин-теллекта, представила GPT-3 [11] — модель, которая в состоянии «писать» це-лые абзацы осмысленного текста. Компания проводила исследования, в ходе которых обычные люди и даже профессионалы не могли понять кто написал этот текст – робот или человек. По результатам впечатляющих исследований, компания приняла решение не предоставлять открытый доступ к данной нейронной сети, так как с её помощью вполне вероятно могли бы создаваться поддельные новости и другие небылицы, а это уже грозило бы судебными раз-бирательствами.
Одним из примеров взаимодействия синтетических данных и GAN-сетей является SimGAN [12]. Компания Apple обучила модель на синтетических дан-ных, улучшенных при помощи SimGAN. Суть была такова: они сделали 3D-модель глаза и с помощью GAN-сетей сделали более реалистичное изображе-ние.
 
Рисунок 1.4 - Пример работы SimGAN
1.6   Среда разработки
Основная среда разработки была Jupyter Notebook [13]. Это бесплатная научная среда с открытым программным кодом, которая предназначена для ра-боты с большими данными, глубоким обучением и построением математиче-ских моделей. Чтобы запустить её, нам нужно установить библиотеку jupyter в Python и после она запускается на локальном хосте. Весь интерфейс состоит из ячеек, которые можно запускать в произвольном порядке. Здесь можно делать все основные этапы разработки моделей для глубокого обучения: подготовку, очистку, трансформацию, тренировку и визуализацию данных.

 
Рисунок 1.5 - Интерфейс Jupyter Notebook

Один из основных минусов — это отсутствие отладчика кода, но это лег-ко исправляется с помощью установки дополнительных модулей.
1.7   Язык разработки
В качестве основного языка программирования выступает язык програм-мирования «Python» [14] или питон.
Python появился в 90-ых как скриптовый язык программирования, но с каждым годом становился все мощнее и популярнее. Теперь на нем можно де-лать все, начиная от веб-приложений и заканчивая анализом данных. В сфере глубокого обучения и всем, что связанно с машинным обучением, питон зани-мает первое место среди разработчиков. Все основные фреймворки для работы с глубоким обучением написаны для Python, один из них это PyTorch.
PyTorch — это библиотека для работы с машинным обучением от компа-нии Facebook. С помощью него можно легко создавать нейронные сети и тре-нировать свои модели. Он позволяет импортировать модели на сервер, умень-шать модели и запускать на мобильных устройствах, а также производить кван-товые вычисления. Вычисления внутри библиотеки происходит с помощью графов состояний.
 
2   Основные компоненты глубокого обучения

2.1   SinGAN
В моей выпускной работе я буду реализовать сеть SinGAN [15] — это сеть, которая умеет делать из одного изображения разные образцы по соотно-шению сторон, а также может анимировать статичное изображение. На рисунке 2.1 показано как из самого левого столбца сеть может сгенерировать разные изображения. С помощью неё можем по единому образцу генерировать контент с разным соотношением сторон и, самое главное, сохраняя структуру изображе-ния, то есть важные атрибуты изображения. Эта сеть отлично иллюстрирует как можно манипулировать с изображениями.
 
Рисунок 2.1 - Пример работы сети
2.2   Архитектура сети
В этой главе рассмотрим общую архитектуре сети, а в последующих все внутренности сети. При построении глубокой сети самым важным аспектом яв-ляется архитектура. Какие будут внутренние слои и как они будут взаимодей-ствовать между собой. Есть входной слой, куда подается входной сигнал, есть выходной слой, откуда снимается результат работы нейросети, и между ними есть скрытые слои. Если скрытых слоев больше, чем 1, нейросеть считается глубокой, если 1, то неглубокой. Глубокой нейронной сеткой является сетка, у которой больше одного скрытого слоя. В моей работе разберем архитектуру сеть SinGAN.
SinGAN в значительной степени универсальный метод, который может из одной картинки сгенерировать анимацию или сделать другое разрешение изоб-ражения. Кроме того, обучение выполняется с одним изображением. Архитек-тура SinGAN имеет структуру пирамиды, состоящую из N слоев, и модель обу-чается, начиная с нижнего слоя и заканчивая последним верхнем слоем. Дело в том, что этап обучения делится на N этапов. Масштаб изображения, вводимого в каждый слой, отличается, и масштаб постепенно увеличивается по сравнению с изображением меньшего масштаба.
Алгоритм по методу SinGAN:
1.	Вводится шум z_N в генератор G_N, чтобы создать первый образец x_N.
2.	Повысим размер x_N и введем в G_(N-1) вместе с шумом z_(N-1) для со-здания нового образца изображения x_1.
3.	Повторите это до G_0.
4.	Изображение с каждого G образца сравнивается с реальным изображение через дискриминатор.
В итоге, из шума создается новое изображение с основными особенностями атрибутами исходного изображения.
 
Рисунок 2.2 - Архитектура сети
2.3   Архитектура Генератора
Генератор отвечает за создание изображений из гауссовского шума. Все бло-ки генератора имеют похожую архитектуру, которая состоит из Гауссовского Шума, Свёрточного слоя, Батч-нормализации и Leaky-Relu, о каждом из эти Х методов подробней:
-	Гауссовский шум (Zn) - складывается с картинкой из пред. итерации, если предыдущей операции не было, то подается только шум, важно, чтобы пода-вался шум до сверхточных слоев, тогда точно уверены, что сеть не забудет про шум.
-	Свёрточные слои (Convolutional Layers) — это операция перемножения ядра свертки на изображение. Ядро проходит над изображением, поэлементно вы-полняя операцию умножения с той частью входных данных, над которой оно сейчас находится, и затем суммирует все полученные значения в один вы-ходной пиксель. Это нужно для того, чтобы определить признаки или сущ-ности на картинки, которые будут объединены для получения нового при-знака на выходе. Цель этого слоя в генерации на каждом шаге, пропущенных деталей.
-	Батч-нормализация (Batch Normalization) - этот слой используют для умень-шения времени тренировки модели. Метод решает следующую проблему, препятствующую эффективному обучению нейронных сетей: по мере рас-пространения сигнала по сети, даже если нормализовали его на входе, пройдя через внутренние слои, он может сильно исказиться как по математическому ожиданию, так и по дисперсии (данное явление называется внутренним кова-риационным сдвигом), что чревато серьезными несоответствиями между градиентами на различных уровнях. Из-за чего используем батч-нормализацию, которая делает очень простое решение: нормализует входные данные таким образом, чтобы получить нулевое матожидание и единичную дисперсию. Нормализация выполняется перед входом в каждый слой.
-	Leaky-Relu - Она вычисляет функцию f(x) = max(0.01, x). Это решает про-блему обнуления градиента для положительных чисел и очень просто вы-числяется.
 
Рисунок 2.3 - Архитектура генератора
2.4   Архитектура Дискриминатора
Архитектура дискриминатора представляется из себя то же самое, что и архитектура генератора только без использования шума. Подаем на вход сети сгенерированное изображение и то же самое делаем с настоящей, потом сравни-ваем их и считаем общую ошибку или функция потерь, которая передается в генератор.
2.5   Функция потерь
Функция потерь — это основная формула, отвечающая за генерацию изоб-ражения, с помощью нее сеть учится и пытается уменьшить ошибку. Она состо-ит из двух подфункций:
-	Ladv - штраф-функция, которая считает на сколько отличается сгенериро-ванное изображение от настоящего.
-	Lrec - смотрит, чтобы все важные элементы картинки сохранились.
 
Рисунок 2.4 - Формула функции-потерь

Нам нужно убедиться, что сгенерированное изображение не сильно отли-чается от оригинала. Для этого используем Lrec. поэтому должны обнулить шум, который подаем генератору, получить изображение, и вычесть из ориги-нальной по квадрату, формула представлена на рисунке 2.5.

 
Рисунок 2.5 - Формула реконструктивной функции потерь
 
3   Разработка

3.1   Постановка задачи
Задача - разработать нейронную сеть SinGAN, которая сможет анимиро-вать изображении и создавать изображения с разным соотношением сторон.
3.2   Реализация архитектуры
Самое первое, что нужно сделать — это написать архитектуру сети. Архи-тектура сети — это основная вещь в глубоком обучении.
Для реализации создадим файл под названием models.py, в нем необходи-мо реализовать два класса с генератором (рис 3.1) и дискриминатором (рис 3.2).
В методе __init__ инициализируем переменные, которые будем использо-вать, а метод forward отвечает за то, в какой последовательности будут исполь-зованы слои нейронной сети; это стандартная практика для классов с глубокими сетями.

 
Рисунок 3.1 - Реализация класса генератор
 
Рисунок 3.2 - Реализация класса дискриминатора
Также создали блок со свёрточными сетями и называли его ConvBlock (рис. 3.3), где вызываем уже реализованные методы из PyTorch, такие как Conv2d, BatchNorm2d, LeakyReLU. Это класс вызывается в генераторе и дис-криминаторе.
 
Рисунок 3.3 - Реализация класса ConvBlock
3.3   Реализация штрафов градиента
Создали дополнительную функцию для штрафования градиента (рис 3.4) или, как его еще называют, WGAN-GP [16]. Суть в том, что штрафуем дискри-минатор за ошибки, если он не угадал, где реальное изображение. Это сделано для того, чтобы стабилизировать обучение в GAN-сетях. Обычно используется бинарная перекрестная энтропия [17], но она дает плохую сходимость генерато-ра и дискриминатора из-за чего обучение нестабильно, поэтому лучше исполь-зовать WGAN_GP. Этот штраф пытается максимизировать разницу между оценками для реальных и сгенерированных изображений, при этом реальные изображения получают более высокую оценку.
При использовании такой функции дискриминатор обучается сходимости, что обеспечивает правильное изменение градиентов, используемых для обнов-ления генератора. Благодаря такому подходу дискриминатор не становится слишком “сильным”, и это позитивно сказывается на генерации изображений. Стандартные подходы WGAN_GP тренировки, должны обучить несколько раз дискриминатор между обновлениями генератора. Типичное соотношение: пять обновлений дискриминатора на одно обновление генератора.
В самом начале создаем число alpha, это одно число в диапазоне от 0 до 1, после это число расширяем с помощью метода expand() до размеров настоящего изображения и передаем на GPU. Далее считаем интерполяцию изображения, смотрим значение дискриминатора и считаем значение градиента c помощью метода torch.autograd.grad(), который автоматически считает сумму градиен-тов.

 
Рисунок 3.4 - Функция штрафов WGAN-GP

Функция потерь штрафа за градиент измеряется, как квадрат разности между нормой градиента прогнозов для входных изображений и 1.
3.4   Реализация одного шага обучения
Создадим файл training.py в котором будет происходить тренировка, но сначала нужно реализовать один шаг обучения, а после запускать полный цикл тренировки.
В начале реализуем оптимизаторы и коэффициенты скорости обучения. В качестве оптимизаторов будем использовать Адам [18], который можно исполь-зовать вместо классической процедуры стохастического градиентного спуска для итеративного обновления весов сети на основе обучающих данных. Адам является эффективным методом и можем быстрее достичь нужной нам сходи-мости. Далее реализуем коэффициент скорости обучения или learning rate [19], c помощью этого коэффициента можем решить, с какой скоростью нам двигаться по градиентному спуску, все эти функции являются стандартными в PyTorch и объявим их для Генератора и Дискриминатора.
 
Рисунок 3.5 - Оптимизаторы и коэффициенты скорости обучения
Еще нужно реализовать обновление сетей или, как еще их называют, Функции потерь — это функции, которые помогают учиться алгоритму, то есть передают градиент в нейронную сеть и пытаются оптимизировать эту функцию для лучших результатов.
Начнем с обновления генератора, считаем с помощью реконструкционной функции потерь (Глава 2.4). В самом начале делаем зануление градиента с по-мощью netG.zero_grad() это нужно делать перед стартом обратной распростра-нения ошибки, это сделано потому что PyTorch накапливает градиент при по-следующих обратных проходах. Далее смотрим какую оценку даст дискримина-тор на наше сгенерированное изображение и считаем среднее значение оценок. Далее считаем обратную ошибку и делаем реконструкционную функцию по-терь, где в конце вызываем метод detach(), так как PyTorch отслеживает все операции, которые связаны с тензором и записывает их граф, то с помощью этого метода, можем прекратить отслеживания и записывание в граф, помогает оптимально использовать память компьютера, таким образом позволяет рабо-тать с миллионами данных. С помощью метода step() обновляем оптимизатор Генератора.
 
Рисунок 3.6 - Обновление генератора
Обновление дискриминатора происходит по похожей схеме, что и генера-тора, только тут не считаем реконструкционную функцию потерь, а считаем с помощью среднеквадратичной ошибкой и корня, между настоящим изображе-ние и сгенерированным. Еще вызываем функцию штрафов градиента functions.calc_gradient_penalty() и считаем распространение обратной ошибки для нее. Далее складываем значения штрафа со значением дискриминатора по сгенерированными и настоящим изображениям, из этого значения получается функция потерь дискриминатора.
 
Рисунок 3.7 - Обновление дискриминатора
3.5   Реализация тренировки
Одна из важнейших функций – это функция обучения (train), без нее ни-каких результатов не может быть. Процесс тренировки прост: начинаем с ма-ленького размера изображения и постепенно увеличиваем размер пока не до-стигнем заданного нами значения (в нашем случаи это 250 на 250 пикселей). Опишу на примере, начинаем с картинки 25 на 25 пикселей и каждую итерацию прогона по нашей сети будем увеличивать на 25 пикселей по всем соотношени-ям, следовательно, нам нужно 10 таких увеличений, чтобы программа остано-вилась. Так же внутри этой функции происходит пересчет градиентов, с помо-щью функции train_single_scale о которых было рассказано в главе 3.4.
 
Рисунок 3.8 - Реализация тренировки
Если будем уменьшать количество уменьшений и прибавлять больше пикселей на каждой итерации, то у нас могут получиться плохие изображения, отличные от исходного образца. На рисунке 3.7 показана зависимость от коли-чества от увеличений изображения, чем меньше увеличений делаем, тем хуже сгенерированный образец. Поэтому оптимальный выбор, это около 8-10 увели-чений шума, который поступает на вход генератору.
 
Рисунок 3.9 - Зависимость качества картинки от увеличения
3.6   Реализация анимации
Процесс анимации очень прост, создаем файл animation.py который будет вызываться через командную строку, где должны указать имя файла, фотогра-фии на которой будем обучать. Листинг тренировки показан на рисунке 3.6, где в начале создаем директорию для сохранения наших результатов. После выби-раем либо тренировку, либо загрузку прошлой тренировки, если такая директо-рия уже существует с тренировочными файлами. После тренировки генерирует-ся gif-файл с несколькими степенями анимации от самой слабой до самой силь-ной.
 
Рисунок 3.10 - Реализация запуска анимации
3.7   Реализация генерации изображений
Чтобы сгенерировать изображения в начале генерируем шум, в зависимо-сти от того какого размера шум сделаем - такая и будет генерироваться изобра-жение, будет генерироваться 50 экземпляров, из которых сможем выбрать по-том лучшие. Генерируем из шума с помощью генератора и потом сохраняем сгенерированный образец.
 
Рисунок 3.11 - Реализация генерации изображения
3.8   Реализация картинок с разным соотношением сторон
Создадим дополнительный файл для запуска генерации изображений че-рез терминал, создаем флаг mode через который будем указывать, что именно хотим генерировать, если выбрали random_samples, то генерируем образцы с таким же размером как, и исходное, а если random_samples_arbitraty_sizes, то задаем какого размера должны генерироваться изображения.
 
Рисунок 3.12 - Реализация запуска генерации через терминал
3.9   Результаты
Для тестирования сети возьмем два изображения. Первое изображение (рис 3.13) прогоним через сеть с помощью GPU или Графического процессора от компании Nvidia 2070 Super.  Не будем выставлять никаких дополнительных параметров, а просто сгенерируем 50 образцов с разной степенью генерации.
 
Рисунок 3.13 - Тестовое изображение №1

Так после одного часа обучения получаем 3 директории. При самой сла-бой генерации там заметны несильно заметна разница, а при самом высокой ге-нерации (рис. 3.14), можем увидеть разницу, как сеть уменьшает крону дерева или просто сжимает само дерево, делая его ниже.
 
Рисунок 3.14 - Результаты после самой сильной генерации
Второе изображение (рис. 3.15), будем генерировать с разным соотноше-нием сторон.
 
Рисунок 3.15 - Тестовое изображение №2

У нас исходное изображение размером 250 на 250 пикселей, хотим попро-бовать сделать это изображение шире, к примеру 325 на 175, или сделать выше 175 на 325, и можем так сделать с помощью сети, все результаты показаны на рисунке 3.16.
 
Рисунок 3.16 - Результаты работы генерации по высоте и ширине
Работа с анимацией занимает такое же количество времени и формирует 12 gif-файлов с разной степенью анимации.









Заключение
В ходе выполнения данной бакалаврской работы были решены поставленные задачи и получены следующие результаты:
1)	изучена предметная область, освоен язык программирования Python, а также среда разработки Jupyter Notebook;
2)	осуществлена постановка задачи ВКР;
3)	спроектирован план разработки, согласно предъявляемым требованиям;
4)	разобрана архитектура;
5)	реализована архитектура SinGAN;
6)	произведено тестирование;
7)	написана документация разработанного приложения;
В ходе выполнения поставленных задач была успешно достигнута поставленная цель работы, т. е. разработана сеть SinGAN, которая может анимировать изоб-ражении и генерировать отличительные от оригинала изображения по соотно-шению сторон, так и по контенту.
Теперь у производителей контента не должно возникнуть проблем с адаптацией контента под разные экраны устройств.
Таким образом, была достигнута основная цель, которая была упомянута во введении, и как следствие, можем считать, что цель выпускной квалификацион-ной работы была достигнута в полном объёме.
Пояснительная записка выполнена в соответствии с ОС ТУСУР 01–2013 “Рабо-ты студенческие по направлениям подготовки и специальностям технического профиля”.

Список использованных источников
1.	Генеративное Моделирование и AI [Электронный ресурс] - Режим досту-па: https://habr.com/ru/post/347184/
2.	Deep Learning [Электронный ресурс] - Режим доступа: https://en.wikipedia.org/wiki/Deep_learning
3.	General Data Protection Regulation [Электронный ресурс] - Режим доступа: https://gdpr-info.eu
4.	Ian Goodfellow [Электронный ресурс] - Режим доступа: https://en.wikipedia.org/wiki/Ian_Goodfellow
5.	Generative Adversarial Networks [Электронный ресурс] - Режим доступа: https://arxiv.org/abs/1406.2661
6.	Vanilla GAN Architecture [Электронный ресурс] - Режим доступа: https://financeandriskblog.accenture.com/analytics/how-generative-adversarial-networks-can-impact-banking
7.	Computer Vision [Электронный ресурс] - Режим доступа: https://en.wikipedia.org/wiki/Computer_vision
8.	StyleGAN 2 [Электронный ресурс] - Режим доступа: https://github.com/NVlabs/stylegan2.git
9.	This Person Does Not Exist [Электронный ресурс] - Режим доступа: https://thispersondoesnotexist.com
10.	OpenAI [Электронный ресурс] - Режим доступа: http://openai.com
11.	GPT-3 [Электронный ресурс] - Режим доступа: https://openai.com/blog/openai-api/
12.	Learning from Simulated and Unsupervised Images through Adversarial Train-ing [Электронный ресурс] - Режим доступа: https://arxiv.org/abs/1612.07828
13.	Jupyter Notebook [Электронный ресурс] - Режим доступа: https://jupyter.org
14.	Python [Электронный ресурс] - Режим доступа: https://www.python.org
15.	SinGAN Learning a Generative Model from a Single Natural Image [Элек-тронный ресурс] - Режим доступа: https://arxiv.org/abs/1905.011
16.	WGAN-GP [Электронный ресурс] - Режим доступа: https://arxiv.org/pdf/1704.00028.pdf
17.	Cross entropy [Электронный ресурс] - Режим доступа:  https://en.wikipedia.org/wiki/Cross_entropy 
18.	Adam: A Method for Stochastic Optimization [Электронный ресурс] - Режим доступа: https://arxiv.org/abs/1412.6980
19.	Learning rate [Электронный ресурс] - Режим доступа: https://en.wikipedia.org/wiki/Learning_rate 


